{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets about eating disorders\n",
    "## Modelling with CV\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download\n",
    "from ast import literal_eval\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence-md4.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval},error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import regex as re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import io\n",
    "\n",
    "punctuations = \"¡!#$%&'()*+,-./:;<=>¿?@[\\]^_`{|}~\"\n",
    "def read_txt(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            list.append(str(line).replace('\\n', ''))\n",
    "    return list\n",
    "\n",
    "stopwords = read_txt('english_stopwords.txt')\n",
    "stemmer = SnowballStemmer('english')\n",
    "def clean_accents(tweet):\n",
    "    tweet = re.sub(r\"[àáâãäå]\", \"a\", tweet)\n",
    "    tweet = re.sub(r\"ç\", \"c\", tweet)\n",
    "    tweet = re.sub(r\"[èéêë]\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"[ìíîï]\", \"i\", tweet)\n",
    "    tweet = re.sub(r\"[òóôõö]\", \"o\", tweet)\n",
    "    tweet = re.sub(r\"[ùúûü]\", \"u\", tweet)\n",
    "    tweet = re.sub(r\"[ýÿ]\", \"y\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet, stem = False):\n",
    "    tweet = tweet.lower().strip()\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'http?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'\\s([@#][\\w_-]+)', \"\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
    "    tweet = clean_accents(tweet)\n",
    "    tweet = re.sub(r\"\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|x+d+[x*d*]*|a*ja+[j+a+]+)\\b\", \"<risas>\", tweet)\n",
    "    for symbol in punctuations:\n",
    "        tweet = tweet.replace(symbol, \"\")\n",
    "    tokens = []\n",
    "    for token in tweet.strip().split():\n",
    "        if token not in punctuations and token not in stopwords:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tid\ttext_orig\tED_Patient\tProED\tinformative\tscientific\thashtags\t\n",
    "# entities_instances_wikidata\tspacy_entities_ids\tspacy_entities_labels\t\n",
    "# falcon_spacy_entities\tfalcon_spacy_labels\tfalcon_spacy_embeddingsmd2_mw50_RW\t\n",
    "# falcon_spacy_embeddingsmd2_mw100_RW\tsent_embedding_1\tsent_embedding_2\n",
    "\n",
    "cols = ['id','text_orig','ED_Patient','ProED','informative','scientific','hashtag','entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels','falcon_spacy_entities'\n",
    "       ,'falcon_spacy_labels','falcon_spacy_embeddingsmd2_mw50_RW','falcon_spacy_embeddingsmd2_mw100_RW','sent_embedding_1','sent_embedding_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = tweets.copy()\n",
    "# .drop(['entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels'], axis=1)\n",
    "\n",
    "tweets1['text_cleaned'] = tweets['text_orig'].apply(lambda s : clean_tweet(s))\n",
    "#print(tweets1['text_cleaned'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "df = tweets.copy()\n",
    "X = df['text_orig']\n",
    "X_b = np.array(df['sent_embedding_1'].tolist())\n",
    "X_b = np.asarray(X_b, dtype=np.float32)\n",
    "\n",
    "\n",
    "#X = np.array(df['falcon_spacy_labels'].tolist())\n",
    "\n",
    "Y1 = df['ED_Patient']\n",
    "Y2 = df['ProED']\n",
    "Y3 = df['informative']\n",
    "Y4 = df['scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AutoModel, AutoTokenizer\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "c_model = [\"bert-base-uncased\",\"vinai/bertweet-base\",\"roberta-base\",\"distilbert-base-cased\", \"camembert-base\", \"albert-base-v1\", \"flaubert/flaubert_base_cased\"]\n",
    "\n",
    "sentence_embedding_l_l = []\n",
    "\n",
    "for nmodel in c_model:\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(nmodel, use_fast=False)\n",
    "\n",
    "    tokenized_text_l = []\n",
    "    indexed_tokens_l = []\n",
    "    segments_ids_l = []\n",
    "    tokens_tensor_l = []\n",
    "    segments_tensors_l = []\n",
    "    token_embeddings_l = []\n",
    "    token_vecs_cat_l = []\n",
    "    sentence_embedding_l = []\n",
    "\n",
    "    for sen in range(0, len(X)):\n",
    "        marked_text = \"[CLS] \" + X[sen] + \" [SEP]\"\n",
    "        tokenized_text = tokenizer.tokenize(marked_text)\n",
    "        tokenized_text_l.append(tokenized_text)\n",
    "\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        indexed_tokens_l.append(indexed_tokens)\n",
    "\n",
    "        segments_ids = [sen] * len(tokenized_text)\n",
    "        segments_ids_l.append(segments_ids)\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        tokens_tensor_l.append(tokens_tensor)\n",
    "        segments_tensors_l.append(segments_tensors)\n",
    "\n",
    "        # Load pre-trained model (weights)\n",
    "        model = AutoModel.from_pretrained(nmodel,\n",
    "                                          output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                          )\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Run the text through BERT, and collect all of the hidden states produced\n",
    "        # from all 12 layers. \n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "            # Evaluating the model will return a different number of objects based on \n",
    "            # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "            # becase we set `output_hidden_states = True`, the third item will be the \n",
    "            # hidden states from all layers. See the documentation for more details:\n",
    "            # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "\n",
    "        # Concatenate the tensors for all layers. We use `stack` here to\n",
    "        # create a new dimension in the tensor.\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "        token_embeddings.size()\n",
    "\n",
    "\n",
    "\n",
    "        # Remove dimension 1, the \"batches\".\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings.size()\n",
    "\n",
    "        # Swap dimensions 0 and 1.\n",
    "        token_embeddings = token_embeddings.permute(1,0,2)\n",
    "        token_embeddings.size()\n",
    "\n",
    "        token_embeddings_l.append(token_embeddings)\n",
    "\n",
    "\n",
    "        # Stores the token vectors, with shape [22 x 3,072]\n",
    "        token_vecs_cat = []\n",
    "\n",
    "        # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "        # For each token in the sentence...\n",
    "        for token in token_embeddings:\n",
    "\n",
    "            # `token` is a [12 x 768] tensor\n",
    "\n",
    "            # Concatenate the vectors (that is, append them together) from the last \n",
    "            # four layers.\n",
    "            # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "            cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "            # Use `cat_vec` to represent `token`.\n",
    "            token_vecs_cat.append(cat_vec)\n",
    "\n",
    "        #print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n",
    "\n",
    "\n",
    "        # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "        # `token_vecs` is a tensor with shape [22 x 768]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "\n",
    "        # Calculate the average of all 22 token vectors.\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        sentence_embedding_l.append(sentence_embedding)\n",
    "        \n",
    "    sentence_embedding_l_l.append(sentence_embedding_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc_l = []\n",
    "\n",
    "\n",
    "for idx,model in enumerate(c_model):\n",
    "    Xc = np.zeros(1968)\n",
    "    for i in range(1968):\n",
    "        Xc[i] = np.concatenate((sentence_embedding_l[i],X_b[i]))\n",
    "    Xc_l.append(Xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets1.copy()\n",
    "\n",
    "X_orig = df['text_orig']\n",
    "X_b = np.array(df['sent_embedding_1'].tolist())\n",
    "X_b = np.asarray(X_b, dtype=np.float32)\n",
    "\n",
    "Y1 = df['ED_Patient']\n",
    "Y2 = df['ProED']\n",
    "Y3 = df['informative']\n",
    "Y4 = df['scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=1000\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,100,input_length=max_len)(inputs)\n",
    "    layer = LSTM(868)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Xc ni Xc_l: \n",
    "    # CATEGORY 1\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "    max_words = 1000\n",
    "    max_len = 868\n",
    "    #tok = Tokenizer(num_words=max_words)\n",
    "    #tok.fit_on_texts(X1_train)\n",
    "    #sequences = tok.texts_to_sequences(X1_train)\n",
    "    sequences_matrix = X1_train\n",
    "    model = RNN()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "              validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "    #test_sequences = X1_test\n",
    "    test_sequences_matrix = X1_test\n",
    "    accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Xc ni Xc_l:\n",
    "    # CATEGORY 2\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "    max_words = 1000\n",
    "    max_len = 868\n",
    "    #tok = Tokenizer(num_words=max_words)\n",
    "    #tok.fit_on_texts(X1_train)\n",
    "    #sequences = tok.texts_to_sequences(X1_train)\n",
    "    sequences_matrix = X1_train\n",
    "    model = RNN()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "              validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "    #test_sequences = X1_test\n",
    "    test_sequences_matrix = X1_test\n",
    "    accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Xc ni Xc_l:\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "    max_words = 1000\n",
    "    max_len = 200\n",
    "    #tok = Tokenizer(num_words=max_words)\n",
    "    #tok.fit_on_texts(X1_train)\n",
    "    #sequences = tok.texts_to_sequences(X1_train)\n",
    "    sequences_matrix = X1_train\n",
    "    model = RNN()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "              validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "    #test_sequences = X1_test\n",
    "    test_sequences_matrix = X1_test\n",
    "    accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORY 4\n",
    "for Xc ni Xc_l:\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "    max_words = 1000\n",
    "    max_len = 20\n",
    "    #tok = Tokenizer(num_words=max_words)\n",
    "    #tok.fit_on_texts(X1_train)\n",
    "    #sequences = tok.texts_to_sequences(X1_train)\n",
    "    sequences_matrix = X1_train\n",
    "    model = RNN()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    model.fit(sequences_matrix,y1_train,batch_size=128,epochs=8,\n",
    "              validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "    #test_sequences = X1_test\n",
    "    test_sequences_matrix = X1_test\n",
    "    accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# CATEGORY 1\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1968\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(Xc),\n",
    "        output_dim=125,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=20)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X1_test,y1_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X1_test,y1_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORY 3\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X1_test,y1_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X1_test,y1_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
