{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Knowledge Graphs and Deep Learning techniques for Categorizing Tweets\n",
    "## Random Forest vs RNN vs Bi-LSTM\n",
    "\n",
    "\n",
    "Authors:\n",
    "\n",
    "Experiments:\n",
    "* Applying RF, RNN and Bi-LSTM models to 2 datasets for classifying 4 binary categories.\n",
    "* 2 datatasets: (i) textual information and (ii) textual information and embeddings obtained from knowledge graph exploitation (KGE).\n",
    " \n",
    " \n",
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download\n",
    "from ast import literal_eval\n",
    "'''\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'entities_instances_wikidata':literal_eval,\n",
    "                            'spacy_entities_ids':literal_eval,\n",
    "                            'spacy_entities_labels':literal_eval,\n",
    "                            'falcon_spacy_entities':literal_eval,\n",
    "                            'falcon_spacy_labels':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd2_mw100_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval,\n",
    "                            'sent_embedding_2':literal_eval},error_bad_lines=False)\n",
    "\n",
    "'''\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence-md4.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval},error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import regex as re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import io\n",
    "\n",
    "punctuations = \"¡!#$%&'()*+,-./:;<=>¿?@[\\]^_`{|}~\"\n",
    "def read_txt(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            list.append(str(line).replace('\\n', ''))\n",
    "    return list\n",
    "\n",
    "stopwords = read_txt('english_stopwords.txt')\n",
    "stemmer = SnowballStemmer('english')\n",
    "def clean_accents(tweet):\n",
    "    tweet = re.sub(r\"[àáâãäå]\", \"a\", tweet)\n",
    "    tweet = re.sub(r\"ç\", \"c\", tweet)\n",
    "    tweet = re.sub(r\"[èéêë]\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"[ìíîï]\", \"i\", tweet)\n",
    "    tweet = re.sub(r\"[òóôõö]\", \"o\", tweet)\n",
    "    tweet = re.sub(r\"[ùúûü]\", \"u\", tweet)\n",
    "    tweet = re.sub(r\"[ýÿ]\", \"y\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet, stem = False):\n",
    "    tweet = tweet.lower().strip()\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'http?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'\\s([@#][\\w_-]+)', \"\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
    "    tweet = clean_accents(tweet)\n",
    "    tweet = re.sub(r\"\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|x+d+[x*d*]*|a*ja+[j+a+]+)\\b\", \"<risas>\", tweet)\n",
    "    for symbol in punctuations:\n",
    "        tweet = tweet.replace(symbol, \"\")\n",
    "    tokens = []\n",
    "    for token in tweet.strip().split():\n",
    "        if token not in punctuations and token not in stopwords:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tid\ttext_orig\tED_Patient\tProED\tinformative\tscientific\thashtags\t\n",
    "# entities_instances_wikidata\tspacy_entities_ids\tspacy_entities_labels\t\n",
    "# falcon_spacy_entities\tfalcon_spacy_labels\tfalcon_spacy_embeddingsmd2_mw50_RW\t\n",
    "# falcon_spacy_embeddingsmd2_mw100_RW\tsent_embedding_1\tsent_embedding_2\n",
    "\n",
    "cols = ['id','text_orig','ED_Patient','ProED','informative','scientific','hashtag','entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels','falcon_spacy_entities'\n",
    "       ,'falcon_spacy_labels','falcon_spacy_embeddingsmd2_mw50_RW','falcon_spacy_embeddingsmd2_mw100_RW','sent_embedding_1','sent_embedding_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = tweets.copy()\n",
    "# .drop(['entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels'], axis=1)\n",
    "\n",
    "tweets1['text_cleaned'] = tweets['text_orig'].apply(lambda s : clean_tweet(s))\n",
    "#print(tweets1['text_cleaned'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features into strings...\n",
      "  DONE.\n",
      "Dataset contains 1,968 samples.\n"
     ]
    }
   ],
   "source": [
    "# This will hold all of the dataset samples, as strings.\n",
    "sen_w_feats = []\n",
    "\n",
    "# The labels for the samples.\n",
    "labels = []\n",
    "\n",
    "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
    "data_df = tweets.copy()\n",
    "\n",
    "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
    "# replace the NaN values with empty string.\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "# Combining features following https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
    "print('Combining features ...')\n",
    "\n",
    "# For each of the samples...\n",
    "for index, row in data_df.iterrows():\n",
    "\n",
    "    # Piece it together...    \n",
    "    combined = row[\"text_orig\"]\n",
    "    combined += \" {:} \".format(row[\"sent_embedding_1\"])\n",
    "    \n",
    "    # Add the combined text to the list.\n",
    "    sen_w_feats.append(combined)\n",
    "\n",
    "    # Also record the sample's label.\n",
    "    labels.append(row[\"ProED\"])\n",
    "\n",
    "print('  DONE.')\n",
    "\n",
    "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data to train and test RF models using original dataset and (original dataset+KGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MICROSOFT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "df = tweets1.copy()\n",
    "X = df['text_cleaned']\n",
    "Xc = sen_w_feats\n",
    "\n",
    "#X = np.array(df['falcon_spacy_labels'].tolist())\n",
    "\n",
    "Y1 = df['ED_Patient']\n",
    "Y2 = df['ProED']\n",
    "Y3 = df['informative']\n",
    "Y4 = df['scientific']\n",
    "\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "documents2 = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(Xc)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(Xc[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents2.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "# Xc is Dataset + KGE information\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "Xc = vectorizer.fit_transform(documents2).toarray()\n",
    "tfidfconverter = TfidfTransformer()\n",
    "Xc = tfidfconverter.fit_transform(Xc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RF Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[272  23]\n",
      " [ 45 251]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       295\n",
      "           1       0.92      0.85      0.88       296\n",
      "\n",
      "    accuracy                           0.88       591\n",
      "   macro avg       0.89      0.89      0.88       591\n",
      "weighted avg       0.89      0.88      0.88       591\n",
      "\n",
      "0.8849407783417935\n",
      "[[253  42]\n",
      " [ 41 255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       295\n",
      "           1       0.86      0.86      0.86       296\n",
      "\n",
      "    accuracy                           0.86       591\n",
      "   macro avg       0.86      0.86      0.86       591\n",
      "weighted avg       0.86      0.86      0.86       591\n",
      "\n",
      "0.8595600676818951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y1, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. RF Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[449  16]\n",
      " [ 20 106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       465\n",
      "           1       0.87      0.84      0.85       126\n",
      "\n",
      "    accuracy                           0.94       591\n",
      "   macro avg       0.91      0.90      0.91       591\n",
      "weighted avg       0.94      0.94      0.94       591\n",
      "\n",
      "0.9390862944162437\n",
      "[[421  44]\n",
      " [ 39  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       465\n",
      "           1       0.66      0.69      0.68       126\n",
      "\n",
      "    accuracy                           0.86       591\n",
      "   macro avg       0.79      0.80      0.79       591\n",
      "weighted avg       0.86      0.86      0.86       591\n",
      "\n",
      "0.8595600676818951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y2, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. RF Applied to Category III - Informatives Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[332  24]\n",
      " [ 50 185]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       356\n",
      "           1       0.89      0.79      0.83       235\n",
      "\n",
      "    accuracy                           0.87       591\n",
      "   macro avg       0.88      0.86      0.87       591\n",
      "weighted avg       0.88      0.87      0.87       591\n",
      "\n",
      "0.8747884940778342\n",
      "[[322  34]\n",
      " [ 59 176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       356\n",
      "           1       0.84      0.75      0.79       235\n",
      "\n",
      "    accuracy                           0.84       591\n",
      "   macro avg       0.84      0.83      0.83       591\n",
      "weighted avg       0.84      0.84      0.84       591\n",
      "\n",
      "0.8426395939086294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y3, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. RF Applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[430   7]\n",
      " [ 28 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96       437\n",
      "           1       0.95      0.82      0.88       154\n",
      "\n",
      "    accuracy                           0.94       591\n",
      "   macro avg       0.94      0.90      0.92       591\n",
      "weighted avg       0.94      0.94      0.94       591\n",
      "\n",
      "0.9407783417935702\n",
      "[[419  18]\n",
      " [ 26 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       437\n",
      "           1       0.88      0.83      0.85       154\n",
      "\n",
      "    accuracy                           0.93       591\n",
      "   macro avg       0.91      0.89      0.90       591\n",
      "weighted avg       0.92      0.93      0.92       591\n",
      "\n",
      "0.9255499153976311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y4, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets1.copy()\n",
    "\n",
    "X_orig = df['text_cleaned']\n",
    "X_b = np.array(df['sent_embedding_1'].tolist())\n",
    "X_b = np.asarray(X_b, dtype=np.float32)\n",
    "\n",
    "Y1 = df['ED_Patient']\n",
    "Y2 = df['ProED']\n",
    "Y3 = df['informative']\n",
    "Y4 = df['scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 100\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_orig)\n",
    "sequences = tok.texts_to_sequences(X_orig)\n",
    "X_c = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((1968,200))\n",
    "for i in range(1968):\n",
    "    X[i] = np.concatenate((X_b[i],X_c[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=1000\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,100,input_length=max_len)(inputs)\n",
    "    layer = LSTM(100)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "'''y1_pred = model.predict(X1_test)\n",
    "y1_pred = np.argmax(y1_pred, axis=1)\n",
    "conf_mat = confusion_matrix(y1_test, y1_pred)'''\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. RNN Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.6772 - acc: 0.6272 - f1_m: 0.4735 - precision_m: 0.6463 - recall_m: 0.4425 - val_loss: 0.6050 - val_acc: 0.7488 - val_f1_m: 0.7158 - val_precision_m: 0.7875 - val_recall_m: 0.6595\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 132ms/step - loss: 0.5020 - acc: 0.7923 - f1_m: 0.7615 - precision_m: 0.8381 - recall_m: 0.7381 - val_loss: 0.4508 - val_acc: 0.8019 - val_f1_m: 0.7791 - val_precision_m: 0.7222 - val_recall_m: 0.8481\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 135ms/step - loss: 0.5440 - acc: 0.8214 - f1_m: 0.8270 - precision_m: 0.8390 - recall_m: 0.8442 - val_loss: 0.4299 - val_acc: 0.7995 - val_f1_m: 0.7468 - val_precision_m: 0.7792 - val_recall_m: 0.7190\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 136ms/step - loss: 0.2864 - acc: 0.8951 - f1_m: 0.8785 - precision_m: 0.8857 - recall_m: 0.8739 - val_loss: 0.4131 - val_acc: 0.8116 - val_f1_m: 0.7749 - val_precision_m: 0.7378 - val_recall_m: 0.8178\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 0.2307 - acc: 0.9148 - f1_m: 0.9088 - precision_m: 0.8982 - recall_m: 0.9221 - val_loss: 0.4138 - val_acc: 0.8116 - val_f1_m: 0.7812 - val_precision_m: 0.7815 - val_recall_m: 0.7842\n",
      "Test set\n",
      "  Loss: 0.339\n",
      "  Accuracy: 0.866\n",
      " fº 0.862\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.6778 - acc: 0.6096 - f1_m: 0.5594 - precision_m: 0.7118 - recall_m: 0.5243 - val_loss: 0.6151 - val_acc: 0.6739 - val_f1_m: 0.7150 - val_precision_m: 0.6255 - val_recall_m: 0.8365\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.5051 - acc: 0.7840 - f1_m: 0.7537 - precision_m: 0.8056 - recall_m: 0.7415 - val_loss: 0.4861 - val_acc: 0.7681 - val_f1_m: 0.7212 - val_precision_m: 0.8323 - val_recall_m: 0.6388\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 0.3894 - acc: 0.8432 - f1_m: 0.8368 - precision_m: 0.8577 - recall_m: 0.8328 - val_loss: 0.4491 - val_acc: 0.7874 - val_f1_m: 0.7718 - val_precision_m: 0.7028 - val_recall_m: 0.8570\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 152ms/step - loss: 0.2807 - acc: 0.8972 - f1_m: 0.8974 - precision_m: 0.8663 - recall_m: 0.9317 - val_loss: 0.4210 - val_acc: 0.8116 - val_f1_m: 0.7778 - val_precision_m: 0.7937 - val_recall_m: 0.7674\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.2220 - acc: 0.9128 - f1_m: 0.9083 - precision_m: 0.9055 - recall_m: 0.9131 - val_loss: 0.4315 - val_acc: 0.8164 - val_f1_m: 0.7892 - val_precision_m: 0.7746 - val_recall_m: 0.8057\n",
      "Test set\n",
      "  Loss: 0.348\n",
      "  Accuracy: 0.856\n",
      " fº 0.857\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "#accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test, y1_test, verbose=0)\n",
    "\n",
    "#print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. RNN Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.7082 - acc: 0.7269 - f1_m: 0.0243 - precision_m: 0.0199 - recall_m: 0.0312 - val_loss: 0.5673 - val_acc: 0.7778 - val_f1_m: 0.0571 - val_precision_m: 0.4167 - val_recall_m: 0.0308\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.4779 - acc: 0.7757 - f1_m: 0.1561 - precision_m: 0.6597 - recall_m: 0.0931 - val_loss: 0.4095 - val_acc: 0.7754 - val_f1_m: 0.0417 - val_precision_m: 0.2500 - val_recall_m: 0.0227\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 140ms/step - loss: 0.3449 - acc: 0.8411 - f1_m: 0.5349 - precision_m: 0.8029 - recall_m: 0.4467 - val_loss: 0.3448 - val_acc: 0.8478 - val_f1_m: 0.6817 - val_precision_m: 0.6229 - val_recall_m: 0.7645\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 141ms/step - loss: 0.2669 - acc: 0.8982 - f1_m: 0.7745 - precision_m: 0.8110 - recall_m: 0.7635 - val_loss: 0.3092 - val_acc: 0.8599 - val_f1_m: 0.6004 - val_precision_m: 0.8595 - val_recall_m: 0.4728\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 0.2097 - acc: 0.9117 - f1_m: 0.8102 - precision_m: 0.8227 - recall_m: 0.8247 - val_loss: 0.2921 - val_acc: 0.8816 - val_f1_m: 0.6599 - val_precision_m: 0.7821 - val_recall_m: 0.5844\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 1s 143ms/step - loss: 0.1647 - acc: 0.9439 - f1_m: 0.8793 - precision_m: 0.8945 - recall_m: 0.8698 - val_loss: 0.3031 - val_acc: 0.8527 - val_f1_m: 0.6972 - val_precision_m: 0.6370 - val_recall_m: 0.7862\n",
      "Test set\n",
      "  Loss: 0.338\n",
      "  Accuracy: 0.843\n",
      " fº 0.677\n",
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 0.6186 - acc: 0.7238 - f1_m: 0.0379 - precision_m: 0.0298 - recall_m: 0.0521 - val_loss: 0.5712 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.4732 - acc: 0.7653 - f1_m: 0.0985 - precision_m: 0.3906 - recall_m: 0.0670 - val_loss: 0.3801 - val_acc: 0.8285 - val_f1_m: 0.4292 - val_precision_m: 0.8854 - val_recall_m: 0.2956\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 177ms/step - loss: 0.3155 - acc: 0.8577 - f1_m: 0.6632 - precision_m: 0.8063 - recall_m: 0.5863 - val_loss: 0.4964 - val_acc: 0.7826 - val_f1_m: 0.1628 - val_precision_m: 0.9167 - val_recall_m: 0.0930\n",
      "Test set\n",
      "  Loss: 0.507\n",
      "  Accuracy: 0.785\n",
      " fº 0.008\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "\n",
    "#print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. RNN Applied to Category III - Informatives Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.6638 - acc: 0.6116 - f1_m: 0.1360 - precision_m: 0.4219 - recall_m: 0.1292 - val_loss: 0.5977 - val_acc: 0.7029 - val_f1_m: 0.3256 - val_precision_m: 0.9792 - val_recall_m: 0.1984\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 155ms/step - loss: 0.5763 - acc: 0.7757 - f1_m: 0.5857 - precision_m: 0.8009 - recall_m: 0.4796 - val_loss: 0.4858 - val_acc: 0.8188 - val_f1_m: 0.7210 - val_precision_m: 0.7750 - val_recall_m: 0.6750\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.3669 - acc: 0.8640 - f1_m: 0.8004 - precision_m: 0.8596 - recall_m: 0.7566 - val_loss: 0.4106 - val_acc: 0.8357 - val_f1_m: 0.7431 - val_precision_m: 0.8742 - val_recall_m: 0.6494\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 157ms/step - loss: 0.2859 - acc: 0.8775 - f1_m: 0.8300 - precision_m: 0.8761 - recall_m: 0.8011 - val_loss: 0.4096 - val_acc: 0.8309 - val_f1_m: 0.7475 - val_precision_m: 0.7899 - val_recall_m: 0.7111\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 157ms/step - loss: 0.2246 - acc: 0.9232 - f1_m: 0.8972 - precision_m: 0.9129 - recall_m: 0.8873 - val_loss: 0.4268 - val_acc: 0.8333 - val_f1_m: 0.7411 - val_precision_m: 0.8225 - val_recall_m: 0.6756\n",
      "Test set\n",
      "  Loss: 0.409\n",
      "  Accuracy: 0.844\n",
      " fº 0.782\n",
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 223ms/step - loss: 0.6634 - acc: 0.6158 - f1_m: 0.0145 - precision_m: 0.0385 - recall_m: 0.0089 - val_loss: 0.6195 - val_acc: 0.6498 - val_f1_m: 0.1495 - val_precision_m: 1.0000 - val_recall_m: 0.0811\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.5408 - acc: 0.7259 - f1_m: 0.4402 - precision_m: 0.9757 - recall_m: 0.3122 - val_loss: 0.4931 - val_acc: 0.8188 - val_f1_m: 0.7084 - val_precision_m: 0.8399 - val_recall_m: 0.6192\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.4168 - acc: 0.8328 - f1_m: 0.7714 - precision_m: 0.8396 - recall_m: 0.7385 - val_loss: 0.4388 - val_acc: 0.8261 - val_f1_m: 0.7455 - val_precision_m: 0.7806 - val_recall_m: 0.7163\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 0.2854 - acc: 0.8868 - f1_m: 0.8418 - precision_m: 0.8821 - recall_m: 0.8109 - val_loss: 0.4429 - val_acc: 0.7971 - val_f1_m: 0.7338 - val_precision_m: 0.7123 - val_recall_m: 0.7673\n",
      "Test set\n",
      "  Loss: 0.390\n",
      "  Accuracy: 0.816\n",
      " fº 0.775\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 3\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. RNN Applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.6421 - acc: 0.7009 - f1_m: 0.0510 - precision_m: 0.0326 - recall_m: 0.1174 - val_loss: 0.5226 - val_acc: 0.7609 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/8\n",
      "8/8 [==============================] - 1s 147ms/step - loss: 0.4819 - acc: 0.7643 - f1_m: 0.0250 - precision_m: 0.1250 - recall_m: 0.0139 - val_loss: 0.4306 - val_acc: 0.7995 - val_f1_m: 0.3478 - val_precision_m: 0.9375 - val_recall_m: 0.2211\n",
      "Epoch 3/8\n",
      "8/8 [==============================] - 1s 149ms/step - loss: 0.3762 - acc: 0.8474 - f1_m: 0.5214 - precision_m: 0.8558 - recall_m: 0.3918 - val_loss: 0.3365 - val_acc: 0.8865 - val_f1_m: 0.7174 - val_precision_m: 0.9410 - val_recall_m: 0.5812\n",
      "Epoch 4/8\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.2351 - acc: 0.9169 - f1_m: 0.8010 - precision_m: 0.8850 - recall_m: 0.7401 - val_loss: 0.3086 - val_acc: 0.8865 - val_f1_m: 0.7510 - val_precision_m: 0.8486 - val_recall_m: 0.6854\n",
      "Epoch 5/8\n",
      "8/8 [==============================] - 1s 151ms/step - loss: 0.1670 - acc: 0.9439 - f1_m: 0.8749 - precision_m: 0.9092 - recall_m: 0.8504 - val_loss: 0.3361 - val_acc: 0.8696 - val_f1_m: 0.7340 - val_precision_m: 0.7956 - val_recall_m: 0.7021\n",
      "Test set\n",
      "  Loss: 0.311\n",
      "  Accuracy: 0.875\n",
      " fº 0.729\n",
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 20, 100)           100000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"inputs_7:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 200).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"inputs_7:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 200).\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7126 - acc: 0.7103 - f1_m: 0.0481 - precision_m: 0.0319 - recall_m: 0.0977WARNING:tensorflow:Model was constructed with shape (None, 20) for input Tensor(\"inputs_7:0\", shape=(None, 20), dtype=float32), but it was called on an input with incompatible shape (None, 200).\n",
      "8/8 [==============================] - 2s 257ms/step - loss: 0.7126 - acc: 0.7103 - f1_m: 0.0481 - precision_m: 0.0319 - recall_m: 0.0977 - val_loss: 0.5363 - val_acc: 0.7609 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.4853 - acc: 0.7612 - f1_m: 0.0125 - precision_m: 0.1250 - recall_m: 0.0066 - val_loss: 0.4407 - val_acc: 0.7995 - val_f1_m: 0.3001 - val_precision_m: 0.9643 - val_recall_m: 0.1783\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.3897 - acc: 0.8432 - f1_m: 0.5248 - precision_m: 0.8438 - recall_m: 0.3954 - val_loss: 0.3435 - val_acc: 0.8647 - val_f1_m: 0.5993 - val_precision_m: 0.9808 - val_recall_m: 0.4332\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.2486 - acc: 0.9024 - f1_m: 0.7830 - precision_m: 0.8777 - recall_m: 0.7260 - val_loss: 0.3187 - val_acc: 0.8865 - val_f1_m: 0.7185 - val_precision_m: 0.9274 - val_recall_m: 0.5897\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.2047 - acc: 0.9283 - f1_m: 0.8281 - precision_m: 0.9003 - recall_m: 0.7874 - val_loss: 0.3188 - val_acc: 0.8865 - val_f1_m: 0.7260 - val_precision_m: 0.9011 - val_recall_m: 0.6138\n",
      "Test set\n",
      "  Loss: 0.295\n",
      "  Accuracy: 0.893\n",
      " fº 0.750\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=8,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Bi-LSTM Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 2s 55ms/step - loss: 0.6893 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6841 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.6591 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6102 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.4925 - acc: 0.6783 - f1_m: 0.4759 - precision_m: 0.6730 - recall_m: 0.3981 - val_loss: 0.4223 - val_acc: 0.7783 - val_f1_m: 0.7256 - val_precision_m: 0.9008 - val_recall_m: 0.6229\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.3836 - acc: 0.8119 - f1_m: 0.7562 - precision_m: 0.8739 - recall_m: 0.6762 - val_loss: 0.3839 - val_acc: 0.8223 - val_f1_m: 0.7934 - val_precision_m: 0.8952 - val_recall_m: 0.7238\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.3292 - acc: 0.8548 - f1_m: 0.8120 - precision_m: 0.8950 - recall_m: 0.7556 - val_loss: 0.3835 - val_acc: 0.8325 - val_f1_m: 0.8019 - val_precision_m: 0.9237 - val_recall_m: 0.7207\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.2800 - acc: 0.8845 - f1_m: 0.8435 - precision_m: 0.9011 - recall_m: 0.7984 - val_loss: 0.3895 - val_acc: 0.8443 - val_f1_m: 0.8151 - val_precision_m: 0.9332 - val_recall_m: 0.7361\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.2478 - acc: 0.8983 - f1_m: 0.8639 - precision_m: 0.9146 - recall_m: 0.8274 - val_loss: 0.3767 - val_acc: 0.8443 - val_f1_m: 0.8229 - val_precision_m: 0.8862 - val_recall_m: 0.7820\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.3158 - acc: 0.8693 - f1_m: 0.8358 - precision_m: 0.8690 - recall_m: 0.8195 - val_loss: 0.3504 - val_acc: 0.8511 - val_f1_m: 0.8279 - val_precision_m: 0.9039 - val_recall_m: 0.7789\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.2330 - acc: 0.9041 - f1_m: 0.8693 - precision_m: 0.9253 - recall_m: 0.8249 - val_loss: 0.3702 - val_acc: 0.8460 - val_f1_m: 0.8183 - val_precision_m: 0.9099 - val_recall_m: 0.7572\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1971 - acc: 0.9245 - f1_m: 0.8931 - precision_m: 0.9328 - recall_m: 0.8609 - val_loss: 0.3895 - val_acc: 0.8545 - val_f1_m: 0.8380 - val_precision_m: 0.8931 - val_recall_m: 0.7993\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1740 - acc: 0.9354 - f1_m: 0.9024 - precision_m: 0.9276 - recall_m: 0.8835 - val_loss: 0.4179 - val_acc: 0.8511 - val_f1_m: 0.8349 - val_precision_m: 0.8899 - val_recall_m: 0.7961\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1546 - acc: 0.9470 - f1_m: 0.9203 - precision_m: 0.9447 - recall_m: 0.9015 - val_loss: 0.4501 - val_acc: 0.8409 - val_f1_m: 0.8205 - val_precision_m: 0.8822 - val_recall_m: 0.7775\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1410 - acc: 0.9521 - f1_m: 0.9272 - precision_m: 0.9446 - recall_m: 0.9143 - val_loss: 0.4708 - val_acc: 0.8477 - val_f1_m: 0.8314 - val_precision_m: 0.8898 - val_recall_m: 0.7895\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1282 - acc: 0.9593 - f1_m: 0.9324 - precision_m: 0.9452 - recall_m: 0.9225 - val_loss: 0.4811 - val_acc: 0.8511 - val_f1_m: 0.8363 - val_precision_m: 0.8864 - val_recall_m: 0.7991\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.4811 - acc: 0.8511 - f1_m: 0.8428 - precision_m: 0.8884 - recall_m: 0.8056\n",
      "Test set\n",
      "  Loss: 0.481\n",
      "  Accuracy: 0.851\n",
      " fº 0.843\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 5s 107ms/step - loss: 0.6897 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6853 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 3s 65ms/step - loss: 0.6676 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6316 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 3s 66ms/step - loss: 0.5512 - acc: 0.6674 - f1_m: 0.4380 - precision_m: 0.7256 - recall_m: 0.3507 - val_loss: 0.4624 - val_acc: 0.7157 - val_f1_m: 0.6003 - val_precision_m: 0.9369 - val_recall_m: 0.4622\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.4441 - acc: 0.7560 - f1_m: 0.6621 - precision_m: 0.8912 - recall_m: 0.5578 - val_loss: 0.4029 - val_acc: 0.7885 - val_f1_m: 0.7385 - val_precision_m: 0.8947 - val_recall_m: 0.6397\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.3564 - acc: 0.8548 - f1_m: 0.8116 - precision_m: 0.8880 - recall_m: 0.7568 - val_loss: 0.3714 - val_acc: 0.8393 - val_f1_m: 0.8266 - val_precision_m: 0.8663 - val_recall_m: 0.7998\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.3178 - acc: 0.8722 - f1_m: 0.8566 - precision_m: 0.9035 - recall_m: 0.8262 - val_loss: 0.3624 - val_acc: 0.8494 - val_f1_m: 0.8371 - val_precision_m: 0.8724 - val_recall_m: 0.8128\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.2781 - acc: 0.8918 - f1_m: 0.8579 - precision_m: 0.8981 - recall_m: 0.8293 - val_loss: 0.3592 - val_acc: 0.8579 - val_f1_m: 0.8411 - val_precision_m: 0.9045 - val_recall_m: 0.7962\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.2479 - acc: 0.9020 - f1_m: 0.8880 - precision_m: 0.9274 - recall_m: 0.8596 - val_loss: 0.3575 - val_acc: 0.8714 - val_f1_m: 0.8610 - val_precision_m: 0.8990 - val_recall_m: 0.8348\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.2226 - acc: 0.9165 - f1_m: 0.8836 - precision_m: 0.9068 - recall_m: 0.8682 - val_loss: 0.3768 - val_acc: 0.8545 - val_f1_m: 0.8353 - val_precision_m: 0.9077 - val_recall_m: 0.7831\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.1973 - acc: 0.9259 - f1_m: 0.9205 - precision_m: 0.9414 - recall_m: 0.9066 - val_loss: 0.3922 - val_acc: 0.8629 - val_f1_m: 0.8483 - val_precision_m: 0.9104 - val_recall_m: 0.8026\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.1770 - acc: 0.9303 - f1_m: 0.9235 - precision_m: 0.9492 - recall_m: 0.9049 - val_loss: 0.3967 - val_acc: 0.8697 - val_f1_m: 0.8620 - val_precision_m: 0.8758 - val_recall_m: 0.8565\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 0.1545 - acc: 0.9463 - f1_m: 0.9212 - precision_m: 0.9359 - recall_m: 0.9111 - val_loss: 0.4419 - val_acc: 0.8528 - val_f1_m: 0.8353 - val_precision_m: 0.8942 - val_recall_m: 0.7951\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.1390 - acc: 0.9484 - f1_m: 0.9226 - precision_m: 0.9444 - recall_m: 0.9073 - val_loss: 0.4426 - val_acc: 0.8494 - val_f1_m: 0.8328 - val_precision_m: 0.8811 - val_recall_m: 0.7999\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.1249 - acc: 0.9528 - f1_m: 0.9292 - precision_m: 0.9446 - recall_m: 0.9191 - val_loss: 0.9643 - val_acc: 0.7530 - val_f1_m: 0.7876 - val_precision_m: 0.6862 - val_recall_m: 0.9406\n",
      "19/19 [==============================] - 0s 23ms/step - loss: 0.9643 - acc: 0.7530 - f1_m: 0.7836 - precision_m: 0.6776 - recall_m: 0.9366\n",
      "Test set\n",
      "  Loss: 0.964\n",
      "  Accuracy: 0.753\n",
      " fº 0.784\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Bi-LSTM Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 4s 97ms/step - loss: 0.6379 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5232 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 3s 57ms/step - loss: 0.5379 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4856 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 3s 58ms/step - loss: 0.4494 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3922 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 2s 56ms/step - loss: 0.3517 - acc: 0.7778 - f1_m: 0.1196 - precision_m: 0.3057 - recall_m: 0.0844 - val_loss: 0.5383 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 3s 58ms/step - loss: 0.3001 - acc: 0.8410 - f1_m: 0.5026 - precision_m: 0.7163 - recall_m: 0.4113 - val_loss: 0.3192 - val_acc: 0.8342 - val_f1_m: 0.4569 - val_precision_m: 0.6651 - val_recall_m: 0.3681\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 2s 56ms/step - loss: 0.2528 - acc: 0.8867 - f1_m: 0.6994 - precision_m: 0.8222 - recall_m: 0.6328 - val_loss: 0.3082 - val_acc: 0.8426 - val_f1_m: 0.4931 - val_precision_m: 0.6687 - val_recall_m: 0.4156\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 3s 58ms/step - loss: 0.2200 - acc: 0.9020 - f1_m: 0.7440 - precision_m: 0.8521 - recall_m: 0.6824 - val_loss: 0.3065 - val_acc: 0.8477 - val_f1_m: 0.5267 - val_precision_m: 0.6437 - val_recall_m: 0.4707\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 2s 55ms/step - loss: 0.1934 - acc: 0.9165 - f1_m: 0.7944 - precision_m: 0.8503 - recall_m: 0.7710 - val_loss: 0.3312 - val_acc: 0.8494 - val_f1_m: 0.4750 - val_precision_m: 0.6528 - val_recall_m: 0.4065\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 3s 59ms/step - loss: 0.1902 - acc: 0.9020 - f1_m: 0.7164 - precision_m: 0.8680 - recall_m: 0.6467 - val_loss: 0.3073 - val_acc: 0.8596 - val_f1_m: 0.6243 - val_precision_m: 0.6904 - val_recall_m: 0.6163\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 0.1604 - acc: 0.9434 - f1_m: 0.8664 - precision_m: 0.8794 - recall_m: 0.8689 - val_loss: 0.3380 - val_acc: 0.8613 - val_f1_m: 0.6019 - val_precision_m: 0.7132 - val_recall_m: 0.5540\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 2s 57ms/step - loss: 0.1448 - acc: 0.9405 - f1_m: 0.8398 - precision_m: 0.9070 - recall_m: 0.8088 - val_loss: 0.3464 - val_acc: 0.8629 - val_f1_m: 0.6023 - val_precision_m: 0.6867 - val_recall_m: 0.5813\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 2s 56ms/step - loss: 0.1262 - acc: 0.9593 - f1_m: 0.9092 - precision_m: 0.9213 - recall_m: 0.9122 - val_loss: 0.3788 - val_acc: 0.8663 - val_f1_m: 0.6088 - val_precision_m: 0.6888 - val_recall_m: 0.5935\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 2s 56ms/step - loss: 0.1102 - acc: 0.9622 - f1_m: 0.8929 - precision_m: 0.9018 - recall_m: 0.8911 - val_loss: 0.4077 - val_acc: 0.8680 - val_f1_m: 0.6207 - val_precision_m: 0.6916 - val_recall_m: 0.6085\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 3s 57ms/step - loss: 0.0996 - acc: 0.9666 - f1_m: 0.9270 - precision_m: 0.9398 - recall_m: 0.9263 - val_loss: 0.4236 - val_acc: 0.8731 - val_f1_m: 0.6659 - val_precision_m: 0.7170 - val_recall_m: 0.6790\n",
      "19/19 [==============================] - 0s 15ms/step - loss: 0.4236 - acc: 0.8731 - f1_m: 0.7082 - precision_m: 0.7524 - recall_m: 0.7297\n",
      "Test set\n",
      "  Loss: 0.424\n",
      "  Accuracy: 0.873\n",
      " fº 0.708\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 3s 72ms/step - loss: 0.6492 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5387 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.4813 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4389 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.4216 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3899 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.3471 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3406 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.2879 - acc: 0.7720 - f1_m: 0.1105 - precision_m: 0.2720 - recall_m: 0.0730 - val_loss: 0.3159 - val_acc: 0.7970 - val_f1_m: 0.1525 - val_precision_m: 0.3833 - val_recall_m: 0.0975\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2350 - acc: 0.8381 - f1_m: 0.5051 - precision_m: 0.8201 - recall_m: 0.3897 - val_loss: 0.3193 - val_acc: 0.8291 - val_f1_m: 0.4448 - val_precision_m: 0.6106 - val_recall_m: 0.3884\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.2000 - acc: 0.9092 - f1_m: 0.7645 - precision_m: 0.8444 - recall_m: 0.7164 - val_loss: 0.3423 - val_acc: 0.8528 - val_f1_m: 0.5517 - val_precision_m: 0.6839 - val_recall_m: 0.5093\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.1728 - acc: 0.9267 - f1_m: 0.7953 - precision_m: 0.8521 - recall_m: 0.7725 - val_loss: 0.3724 - val_acc: 0.8562 - val_f1_m: 0.5494 - val_precision_m: 0.6717 - val_recall_m: 0.5132\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 14ms/step - loss: 0.1526 - acc: 0.9383 - f1_m: 0.8356 - precision_m: 0.8955 - recall_m: 0.7973 - val_loss: 0.3846 - val_acc: 0.8629 - val_f1_m: 0.5863 - val_precision_m: 0.6606 - val_recall_m: 0.5679\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.1310 - acc: 0.9564 - f1_m: 0.8721 - precision_m: 0.9085 - recall_m: 0.8570 - val_loss: 0.3761 - val_acc: 0.8613 - val_f1_m: 0.6143 - val_precision_m: 0.6829 - val_recall_m: 0.6007\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 17ms/step - loss: 0.1091 - acc: 0.9586 - f1_m: 0.8728 - precision_m: 0.9035 - recall_m: 0.8583 - val_loss: 0.4412 - val_acc: 0.8528 - val_f1_m: 0.6080 - val_precision_m: 0.6486 - val_recall_m: 0.6129\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 17ms/step - loss: 0.0941 - acc: 0.9680 - f1_m: 0.9210 - precision_m: 0.9491 - recall_m: 0.9065 - val_loss: 0.5148 - val_acc: 0.8562 - val_f1_m: 0.5693 - val_precision_m: 0.6346 - val_recall_m: 0.5496\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.0811 - acc: 0.9731 - f1_m: 0.9395 - precision_m: 0.9665 - recall_m: 0.9246 - val_loss: 0.5725 - val_acc: 0.8494 - val_f1_m: 0.5726 - val_precision_m: 0.6507 - val_recall_m: 0.5552\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 15ms/step - loss: 0.0650 - acc: 0.9811 - f1_m: 0.9363 - precision_m: 0.9564 - recall_m: 0.9225 - val_loss: 0.6548 - val_acc: 0.8528 - val_f1_m: 0.5682 - val_precision_m: 0.6869 - val_recall_m: 0.5238\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 0.6548 - acc: 0.8528 - f1_m: 0.6176 - precision_m: 0.7376 - recall_m: 0.5662\n",
      "Test set\n",
      "  Loss: 0.655\n",
      "  Accuracy: 0.853\n",
      " fº 0.618\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Bi-LSTM Applied to Category III - Informatives Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.6847 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6740 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 2s 55ms/step - loss: 0.6593 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6530 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 2s 56ms/step - loss: 0.6033 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5780 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 3s 57ms/step - loss: 0.4662 - acc: 0.7357 - f1_m: 0.4359 - precision_m: 0.8007 - recall_m: 0.3314 - val_loss: 0.4424 - val_acc: 0.8088 - val_f1_m: 0.6844 - val_precision_m: 0.9314 - val_recall_m: 0.5644\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 3s 57ms/step - loss: 0.3818 - acc: 0.8388 - f1_m: 0.7091 - precision_m: 0.9067 - recall_m: 0.6024 - val_loss: 0.3963 - val_acc: 0.8409 - val_f1_m: 0.7766 - val_precision_m: 0.8496 - val_recall_m: 0.7357\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.3170 - acc: 0.8577 - f1_m: 0.7601 - precision_m: 0.8549 - recall_m: 0.6938 - val_loss: 0.3894 - val_acc: 0.8460 - val_f1_m: 0.7796 - val_precision_m: 0.8816 - val_recall_m: 0.7171\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.2761 - acc: 0.8707 - f1_m: 0.7901 - precision_m: 0.8602 - recall_m: 0.7407 - val_loss: 0.3975 - val_acc: 0.8393 - val_f1_m: 0.7688 - val_precision_m: 0.8719 - val_recall_m: 0.7090\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 3s 59ms/step - loss: 0.2395 - acc: 0.8947 - f1_m: 0.8299 - precision_m: 0.8944 - recall_m: 0.7842 - val_loss: 0.4315 - val_acc: 0.8409 - val_f1_m: 0.7708 - val_precision_m: 0.8854 - val_recall_m: 0.7033\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.2063 - acc: 0.9070 - f1_m: 0.8706 - precision_m: 0.9306 - recall_m: 0.8287 - val_loss: 0.4406 - val_acc: 0.8409 - val_f1_m: 0.7879 - val_precision_m: 0.8194 - val_recall_m: 0.7836\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.1810 - acc: 0.9187 - f1_m: 0.8677 - precision_m: 0.9095 - recall_m: 0.8370 - val_loss: 0.4834 - val_acc: 0.8511 - val_f1_m: 0.7959 - val_precision_m: 0.8419 - val_recall_m: 0.7810\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.1535 - acc: 0.9383 - f1_m: 0.9176 - precision_m: 0.9559 - recall_m: 0.8906 - val_loss: 0.5332 - val_acc: 0.8376 - val_f1_m: 0.7696 - val_precision_m: 0.8513 - val_recall_m: 0.7307\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 2s 56ms/step - loss: 0.2733 - acc: 0.8874 - f1_m: 0.8124 - precision_m: 0.8837 - recall_m: 0.7771 - val_loss: 0.4390 - val_acc: 0.8173 - val_f1_m: 0.7342 - val_precision_m: 0.8322 - val_recall_m: 0.6721\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.1961 - acc: 0.9201 - f1_m: 0.8657 - precision_m: 0.9342 - recall_m: 0.8144 - val_loss: 0.4429 - val_acc: 0.8359 - val_f1_m: 0.7735 - val_precision_m: 0.8322 - val_recall_m: 0.7435\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 2s 55ms/step - loss: 0.1358 - acc: 0.9477 - f1_m: 0.9050 - precision_m: 0.9454 - recall_m: 0.8723 - val_loss: 0.5053 - val_acc: 0.8376 - val_f1_m: 0.7798 - val_precision_m: 0.8289 - val_recall_m: 0.7609\n",
      "19/19 [==============================] - 0s 14ms/step - loss: 0.5053 - acc: 0.8376 - f1_m: 0.7837 - precision_m: 0.8252 - recall_m: 0.7598\n",
      "Test set\n",
      "  Loss: 0.505\n",
      "  Accuracy: 0.838\n",
      " fº 0.784\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 3s 60ms/step - loss: 0.6820 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6722 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.6443 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6219 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.5426 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4945 - val_acc: 0.6091 - val_f1_m: 0.0333 - val_precision_m: 0.1333 - val_recall_m: 0.0192\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.4245 - acc: 0.7792 - f1_m: 0.5833 - precision_m: 0.8723 - recall_m: 0.4755 - val_loss: 0.4458 - val_acc: 0.8240 - val_f1_m: 0.7646 - val_precision_m: 0.8133 - val_recall_m: 0.7511\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 21ms/step - loss: 0.3450 - acc: 0.8722 - f1_m: 0.7935 - precision_m: 0.8556 - recall_m: 0.7529 - val_loss: 0.4219 - val_acc: 0.8562 - val_f1_m: 0.7947 - val_precision_m: 0.8775 - val_recall_m: 0.7484\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.2779 - acc: 0.8976 - f1_m: 0.8570 - precision_m: 0.9114 - recall_m: 0.8147 - val_loss: 0.4564 - val_acc: 0.8511 - val_f1_m: 0.7888 - val_precision_m: 0.8721 - val_recall_m: 0.7395\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.2486 - acc: 0.9085 - f1_m: 0.8489 - precision_m: 0.9028 - recall_m: 0.8175 - val_loss: 0.4588 - val_acc: 0.8494 - val_f1_m: 0.7915 - val_precision_m: 0.8587 - val_recall_m: 0.7573\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 20ms/step - loss: 0.2011 - acc: 0.9288 - f1_m: 0.8991 - precision_m: 0.9393 - recall_m: 0.8675 - val_loss: 0.4957 - val_acc: 0.8376 - val_f1_m: 0.7685 - val_precision_m: 0.8637 - val_recall_m: 0.7137\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1644 - acc: 0.9441 - f1_m: 0.9242 - precision_m: 0.9600 - recall_m: 0.8966 - val_loss: 0.5494 - val_acc: 0.8409 - val_f1_m: 0.7814 - val_precision_m: 0.8384 - val_recall_m: 0.7546\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1508 - acc: 0.9492 - f1_m: 0.9062 - precision_m: 0.9322 - recall_m: 0.8881 - val_loss: 0.5475 - val_acc: 0.8257 - val_f1_m: 0.7662 - val_precision_m: 0.8079 - val_recall_m: 0.7580\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.1240 - acc: 0.9557 - f1_m: 0.9176 - precision_m: 0.9498 - recall_m: 0.8918 - val_loss: 0.6569 - val_acc: 0.8342 - val_f1_m: 0.7794 - val_precision_m: 0.8041 - val_recall_m: 0.7825\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.0974 - acc: 0.9695 - f1_m: 0.9342 - precision_m: 0.9518 - recall_m: 0.9205 - val_loss: 0.7332 - val_acc: 0.8342 - val_f1_m: 0.7737 - val_precision_m: 0.8047 - val_recall_m: 0.7692\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.0760 - acc: 0.9753 - f1_m: 0.9645 - precision_m: 0.9791 - recall_m: 0.9541 - val_loss: 0.8136 - val_acc: 0.8325 - val_f1_m: 0.7724 - val_precision_m: 0.8080 - val_recall_m: 0.7636\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 19ms/step - loss: 0.0669 - acc: 0.9789 - f1_m: 0.9485 - precision_m: 0.9613 - recall_m: 0.9376 - val_loss: 0.8858 - val_acc: 0.8274 - val_f1_m: 0.7690 - val_precision_m: 0.7811 - val_recall_m: 0.7807\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.8858 - acc: 0.8274 - f1_m: 0.7739 - precision_m: 0.7765 - recall_m: 0.7798\n",
      "Test set\n",
      "  Loss: 0.886\n",
      "  Accuracy: 0.827\n",
      " fº 0.774\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 3\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Bi-LSTM Applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 2s 57ms/step - loss: 0.6499 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5856 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.5391 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5336 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.4661 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4329 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.3677 - acc: 0.7654 - f1_m: 0.0307 - precision_m: 0.1364 - recall_m: 0.0175 - val_loss: 0.3438 - val_acc: 0.7902 - val_f1_m: 0.2795 - val_precision_m: 0.6333 - val_recall_m: 0.1968\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.2748 - acc: 0.9005 - f1_m: 0.7234 - precision_m: 0.9063 - recall_m: 0.6367 - val_loss: 0.3226 - val_acc: 0.8782 - val_f1_m: 0.7315 - val_precision_m: 0.8878 - val_recall_m: 0.6573\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.2085 - acc: 0.9317 - f1_m: 0.8329 - precision_m: 0.9034 - recall_m: 0.7898 - val_loss: 0.2839 - val_acc: 0.8866 - val_f1_m: 0.7524 - val_precision_m: 0.8856 - val_recall_m: 0.6899\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.1664 - acc: 0.9441 - f1_m: 0.8493 - precision_m: 0.8987 - recall_m: 0.8204 - val_loss: 0.2935 - val_acc: 0.8985 - val_f1_m: 0.7682 - val_precision_m: 0.8436 - val_recall_m: 0.7446\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.1391 - acc: 0.9586 - f1_m: 0.8844 - precision_m: 0.9182 - recall_m: 0.8655 - val_loss: 0.2867 - val_acc: 0.9036 - val_f1_m: 0.7738 - val_precision_m: 0.8310 - val_recall_m: 0.7700\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.1145 - acc: 0.9680 - f1_m: 0.9029 - precision_m: 0.9427 - recall_m: 0.8771 - val_loss: 0.3483 - val_acc: 0.8951 - val_f1_m: 0.7669 - val_precision_m: 0.8847 - val_recall_m: 0.7147\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.0992 - acc: 0.9680 - f1_m: 0.9018 - precision_m: 0.9333 - recall_m: 0.8831 - val_loss: 0.3405 - val_acc: 0.8934 - val_f1_m: 0.7548 - val_precision_m: 0.8050 - val_recall_m: 0.7597\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.0859 - acc: 0.9746 - f1_m: 0.9193 - precision_m: 0.9581 - recall_m: 0.8919 - val_loss: 0.3670 - val_acc: 0.9036 - val_f1_m: 0.7702 - val_precision_m: 0.8258 - val_recall_m: 0.7679\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.0684 - acc: 0.9789 - f1_m: 0.9524 - precision_m: 0.9754 - recall_m: 0.9350 - val_loss: 0.4010 - val_acc: 0.8883 - val_f1_m: 0.7493 - val_precision_m: 0.7544 - val_recall_m: 0.7929\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.0526 - acc: 0.9847 - f1_m: 0.9408 - precision_m: 0.9539 - recall_m: 0.9329 - val_loss: 0.4666 - val_acc: 0.8985 - val_f1_m: 0.7469 - val_precision_m: 0.8581 - val_recall_m: 0.7000\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 16ms/step - loss: 0.0590 - acc: 0.9826 - f1_m: 0.9555 - precision_m: 0.9822 - recall_m: 0.9354 - val_loss: 0.4473 - val_acc: 0.8951 - val_f1_m: 0.7504 - val_precision_m: 0.7900 - val_recall_m: 0.7534\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 0.4473 - acc: 0.8951 - f1_m: 0.7570 - precision_m: 0.7987 - recall_m: 0.7378\n",
      "Test set\n",
      "  Loss: 0.447\n",
      "  Accuracy: 0.895\n",
      " fº 0.757\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 5s 106ms/step - loss: 0.6659 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6174 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.5547 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5546 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.5077 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4985 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.4045 - acc: 0.7611 - f1_m: 0.0051 - precision_m: 0.0227 - recall_m: 0.0028 - val_loss: 0.3571 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 0.3489 - acc: 0.8090 - f1_m: 0.2888 - precision_m: 0.6477 - recall_m: 0.1950 - val_loss: 0.3404 - val_acc: 0.7970 - val_f1_m: 0.3271 - val_precision_m: 0.6500 - val_recall_m: 0.2352\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.2681 - acc: 0.8794 - f1_m: 0.6375 - precision_m: 0.9271 - recall_m: 0.5213 - val_loss: 0.2948 - val_acc: 0.8765 - val_f1_m: 0.6556 - val_precision_m: 0.8556 - val_recall_m: 0.5740\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 0.2138 - acc: 0.9165 - f1_m: 0.7970 - precision_m: 0.9472 - recall_m: 0.7051 - val_loss: 0.2704 - val_acc: 0.8985 - val_f1_m: 0.7660 - val_precision_m: 0.8589 - val_recall_m: 0.7406\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.1756 - acc: 0.9281 - f1_m: 0.8049 - precision_m: 0.9171 - recall_m: 0.7301 - val_loss: 0.2891 - val_acc: 0.8934 - val_f1_m: 0.7585 - val_precision_m: 0.8944 - val_recall_m: 0.6988\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.1580 - acc: 0.9375 - f1_m: 0.8281 - precision_m: 0.9268 - recall_m: 0.7684 - val_loss: 0.2847 - val_acc: 0.8900 - val_f1_m: 0.7429 - val_precision_m: 0.8700 - val_recall_m: 0.6891\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.1344 - acc: 0.9528 - f1_m: 0.8667 - precision_m: 0.9462 - recall_m: 0.8172 - val_loss: 0.2971 - val_acc: 0.8883 - val_f1_m: 0.7387 - val_precision_m: 0.8700 - val_recall_m: 0.6835\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.1233 - acc: 0.9528 - f1_m: 0.8601 - precision_m: 0.9538 - recall_m: 0.7986 - val_loss: 0.2911 - val_acc: 0.8951 - val_f1_m: 0.7520 - val_precision_m: 0.8644 - val_recall_m: 0.7053\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.1028 - acc: 0.9593 - f1_m: 0.8697 - precision_m: 0.9221 - recall_m: 0.8395 - val_loss: 0.2898 - val_acc: 0.9019 - val_f1_m: 0.7697 - val_precision_m: 0.8394 - val_recall_m: 0.7500\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 3s 68ms/step - loss: 0.0860 - acc: 0.9666 - f1_m: 0.9278 - precision_m: 0.9687 - recall_m: 0.9005 - val_loss: 0.3239 - val_acc: 0.9002 - val_f1_m: 0.7671 - val_precision_m: 0.8561 - val_recall_m: 0.7348\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 3s 70ms/step - loss: 0.0806 - acc: 0.9644 - f1_m: 0.9030 - precision_m: 0.9427 - recall_m: 0.8784 - val_loss: 0.3393 - val_acc: 0.8917 - val_f1_m: 0.7293 - val_precision_m: 0.8394 - val_recall_m: 0.6786\n",
      "19/19 [==============================] - 0s 20ms/step - loss: 0.3393 - acc: 0.8917 - f1_m: 0.7443 - precision_m: 0.8698 - recall_m: 0.6791\n",
      "Test set\n",
      "  Loss: 0.339\n",
      "  Accuracy: 0.892\n",
      " fº 0.744\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
