{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Knowledge Graphs and Deep Learning techniques for Categorizing Tweets\n",
    "## Random Forest vs RNN vs Bi-LSTM\n",
    "\n",
    "\n",
    "Authors:\n",
    "<!-- ¡ -->\n",
    "\n",
    "Experiments:\n",
    "* Applying RF, RNN and Bi-LSTM models to 2 datasets for classifying 4 binary categories.\n",
    "* 2 datatasets: (i) textual information and (ii) textual information and embeddings obtained from knowledge graph exploitation (KGE).\n",
    " \n",
    " \n",
    "## 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download\n",
    "from ast import literal_eval\n",
    "'''\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'entities_instances_wikidata':literal_eval,\n",
    "                            'spacy_entities_ids':literal_eval,\n",
    "                            'spacy_entities_labels':literal_eval,\n",
    "                            'falcon_spacy_entities':literal_eval,\n",
    "                            'falcon_spacy_labels':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'falcon_spacy_embeddingsmd2_mw100_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval,\n",
    "                            'sent_embedding_2':literal_eval},error_bad_lines=False)\n",
    "\n",
    "'''\n",
    "tweets = pd.read_csv('ed-dataset-falcon_spacy2-embeddings-sentence-md4.csv', sep=';', encoding='utf8', converters=\n",
    "                           {\n",
    "                            'falcon_spacy_embeddingsmd4_mw50_RW':literal_eval,\n",
    "                            'sent_embedding_1':literal_eval},error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import regex as re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import io\n",
    "\n",
    "punctuations = \"¡!#$%&'()*+,-./:;<=>¿?@[\\]^_`{|}~\"\n",
    "def read_txt(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            list.append(str(line).replace('\\n', ''))\n",
    "    return list\n",
    "\n",
    "stopwords = read_txt('english_stopwords.txt')\n",
    "stemmer = SnowballStemmer('english')\n",
    "def clean_accents(tweet):\n",
    "    tweet = re.sub(r\"[àáâãäå]\", \"a\", tweet)\n",
    "    tweet = re.sub(r\"ç\", \"c\", tweet)\n",
    "    tweet = re.sub(r\"[èéêë]\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"[ìíîï]\", \"i\", tweet)\n",
    "    tweet = re.sub(r\"[òóôõö]\", \"o\", tweet)\n",
    "    tweet = re.sub(r\"[ùúûü]\", \"u\", tweet)\n",
    "    tweet = re.sub(r\"[ýÿ]\", \"y\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet, stem = False):\n",
    "    tweet = tweet.lower().strip()\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'http?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'\\s([@#][\\w_-]+)', \"\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
    "    tweet = clean_accents(tweet)\n",
    "    tweet = re.sub(r\"\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|x+d+[x*d*]*|a*ja+[j+a+]+)\\b\", \"<risas>\", tweet)\n",
    "    for symbol in punctuations:\n",
    "        tweet = tweet.replace(symbol, \"\")\n",
    "    tokens = []\n",
    "    for token in tweet.strip().split():\n",
    "        if token not in punctuations and token not in stopwords:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tid\ttext_orig\tED_Patient\tProED\tinformative\tscientific\thashtags\t\n",
    "# entities_instances_wikidata\tspacy_entities_ids\tspacy_entities_labels\t\n",
    "# falcon_spacy_entities\tfalcon_spacy_labels\tfalcon_spacy_embeddingsmd2_mw50_RW\t\n",
    "# falcon_spacy_embeddingsmd2_mw100_RW\tsent_embedding_1\tsent_embedding_2\n",
    "\n",
    "cols = ['id','text_orig','ED_Patient','ProED','informative','scientific','hashtag','entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels','falcon_spacy_entities'\n",
    "       ,'falcon_spacy_labels','falcon_spacy_embeddingsmd2_mw50_RW','falcon_spacy_embeddingsmd2_mw100_RW','sent_embedding_1','sent_embedding_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = tweets.copy()\n",
    "# .drop(['entities_instances_wikidata','spacy_entities_ids','spacy_entities_labels'], axis=1)\n",
    "\n",
    "tweets1['text_cleaned'] = tweets['text_orig'].apply(lambda s : clean_tweet(s))\n",
    "#print(tweets1['text_cleaned'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features into strings...\n",
      "  DONE.\n",
      "Dataset contains 1,968 samples.\n"
     ]
    }
   ],
   "source": [
    "# This will hold all of the dataset samples, as strings.\n",
    "sen_w_feats = []\n",
    "\n",
    "# The labels for the samples.\n",
    "labels = []\n",
    "\n",
    "# First, reload the dataset to undo the transformations we applied for XGBoost.\n",
    "data_df = tweets.copy()\n",
    "\n",
    "# Some of the reviews are missing either a \"Title\" or \"Review Text\", so we'll \n",
    "# replace the NaN values with empty string.\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "# Combining features following https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/\n",
    "print('Combining features ...')\n",
    "\n",
    "# For each of the samples...\n",
    "for index, row in data_df.iterrows():\n",
    "\n",
    "    # Piece it together...    \n",
    "    combined = \" {:} \".format(row[\"sent_embedding_1\"])\n",
    "    \n",
    "    # Add the combined text to the list.\n",
    "    sen_w_feats.append(combined)\n",
    "\n",
    "    # Also record the sample's label.\n",
    "    labels.append(row[\"ProED\"])\n",
    "\n",
    "print('  DONE.')\n",
    "\n",
    "print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data to train and test RF models using original dataset and (original dataset+KGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MICROSOFT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "df = tweets1.copy()\n",
    "X = df['text_cleaned']\n",
    "Xc = sen_w_feats\n",
    "\n",
    "#X = np.array(df['falcon_spacy_labels'].tolist())\n",
    "\n",
    "Y1 = df['ED_Patient']\n",
    "Y2 = df['ProED']\n",
    "Y3 = df['informative']\n",
    "Y4 = df['scientific']\n",
    "\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "documents2 = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(Xc)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(Xc[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents2.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "# Xc is Dataset + KGE information\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "Xc = vectorizer.fit_transform(documents2).toarray()\n",
    "tfidfconverter = TfidfTransformer()\n",
    "Xc = tfidfconverter.fit_transform(Xc).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RF Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[295   0]\n",
      " [292   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       295\n",
      "           1       1.00      0.01      0.03       296\n",
      "\n",
      "    accuracy                           0.51       591\n",
      "   macro avg       0.75      0.51      0.35       591\n",
      "weighted avg       0.75      0.51      0.35       591\n",
      "\n",
      "0.505922165820643\n",
      "[[253  42]\n",
      " [ 43 253]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       295\n",
      "           1       0.86      0.85      0.86       296\n",
      "\n",
      "    accuracy                           0.86       591\n",
      "   macro avg       0.86      0.86      0.86       591\n",
      "weighted avg       0.86      0.86      0.86       591\n",
      "\n",
      "0.856175972927242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y1, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. RF Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[463   2]\n",
      " [124   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88       465\n",
      "           1       0.50      0.02      0.03       126\n",
      "\n",
      "    accuracy                           0.79       591\n",
      "   macro avg       0.64      0.51      0.46       591\n",
      "weighted avg       0.73      0.79      0.70       591\n",
      "\n",
      "0.7868020304568528\n",
      "[[420  45]\n",
      " [ 38  88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       465\n",
      "           1       0.66      0.70      0.68       126\n",
      "\n",
      "    accuracy                           0.86       591\n",
      "   macro avg       0.79      0.80      0.79       591\n",
      "weighted avg       0.86      0.86      0.86       591\n",
      "\n",
      "0.8595600676818951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y2, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. RF Applied to Category III - Informatives Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[343  13]\n",
      " [229   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.96      0.74       356\n",
      "           1       0.32      0.03      0.05       235\n",
      "\n",
      "    accuracy                           0.59       591\n",
      "   macro avg       0.46      0.49      0.39       591\n",
      "weighted avg       0.49      0.59      0.46       591\n",
      "\n",
      "0.5905245346869712\n",
      "[[325  31]\n",
      " [ 61 174]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.88       356\n",
      "           1       0.85      0.74      0.79       235\n",
      "\n",
      "    accuracy                           0.84       591\n",
      "   macro avg       0.85      0.83      0.83       591\n",
      "weighted avg       0.84      0.84      0.84       591\n",
      "\n",
      "0.8443316412859561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y3, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. RF Applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[437   0]\n",
      " [154   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85       437\n",
      "           1       0.00      0.00      0.00       154\n",
      "\n",
      "    accuracy                           0.74       591\n",
      "   macro avg       0.37      0.50      0.43       591\n",
      "weighted avg       0.55      0.74      0.63       591\n",
      "\n",
      "0.739424703891709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MICROSOFT\\Anaconda3\\envs\\tfm2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[418  19]\n",
      " [ 28 126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       437\n",
      "           1       0.87      0.82      0.84       154\n",
      "\n",
      "    accuracy                           0.92       591\n",
      "   macro avg       0.90      0.89      0.89       591\n",
      "weighted avg       0.92      0.92      0.92       591\n",
      "\n",
      "0.9204737732656514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(Xc, Y4, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets1.copy()\n",
    "\n",
    "X_orig = df['text_cleaned']\n",
    "X_b = np.array(df['sent_embedding_1'].tolist())\n",
    "X_b = np.asarray(X_b, dtype=np.float32)\n",
    "\n",
    "Y1 = df['ED_Patient']\n",
    "Y2 = df['ProED']\n",
    "Y3 = df['informative']\n",
    "Y4 = df['scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 100\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X_orig)\n",
    "sequences = tok.texts_to_sequences(X_orig)\n",
    "X_c = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=1000\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,100,input_length=max_len)(inputs)\n",
    "    layer = LSTM(100)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "'''y1_pred = model.predict(X1_test)\n",
    "y1_pred = np.argmax(y1_pred, axis=1)\n",
    "conf_mat = confusion_matrix(y1_test, y1_pred)'''\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. RNN Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.6938 - acc: 0.4911 - f1_m: 0.2344 - precision_m: 0.2581 - recall_m: 0.3246WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "8/8 [==============================] - 1s 117ms/step - loss: 0.6934 - acc: 0.4953 - f1_m: 0.2051 - precision_m: 0.2259 - recall_m: 0.2840 - val_loss: 0.6898 - val_acc: 0.5411 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.6940 - acc: 0.5182 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6909 - val_acc: 0.5411 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.694\n",
      "  Accuracy: 0.499\n",
      " fº 0.000\n",
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 216ms/step - loss: 0.6796 - acc: 0.6075 - f1_m: 0.5390 - precision_m: 0.6305 - recall_m: 0.5708 - val_loss: 0.6281 - val_acc: 0.7560 - val_f1_m: 0.7056 - val_precision_m: 0.7855 - val_recall_m: 0.6471\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 0.5529 - acc: 0.8048 - f1_m: 0.7560 - precision_m: 0.8533 - recall_m: 0.7461 - val_loss: 0.4719 - val_acc: 0.7971 - val_f1_m: 0.7576 - val_precision_m: 0.7831 - val_recall_m: 0.7419\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 0.3605 - acc: 0.8640 - f1_m: 0.8596 - precision_m: 0.8587 - recall_m: 0.8716 - val_loss: 0.4375 - val_acc: 0.7826 - val_f1_m: 0.7358 - val_precision_m: 0.7791 - val_recall_m: 0.7030\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 165ms/step - loss: 0.2607 - acc: 0.9097 - f1_m: 0.9036 - precision_m: 0.9123 - recall_m: 0.8969 - val_loss: 0.4009 - val_acc: 0.8309 - val_f1_m: 0.8175 - val_precision_m: 0.7745 - val_recall_m: 0.8714\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 0.1947 - acc: 0.9304 - f1_m: 0.9264 - precision_m: 0.9225 - recall_m: 0.9314 - val_loss: 0.4376 - val_acc: 0.8285 - val_f1_m: 0.8219 - val_precision_m: 0.7515 - val_recall_m: 0.9096\n",
      "Test set\n",
      "  Loss: 0.359\n",
      "  Accuracy: 0.860\n",
      " fº 0.866\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "#accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test, y1_test, verbose=0)\n",
    "\n",
    "#print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "#accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. RNN Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_3:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_3:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.6007 - acc: 0.7518 - f1_m: 0.0116 - precision_m: 0.0250 - recall_m: 0.0076WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_3:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "8/8 [==============================] - 1s 126ms/step - loss: 0.6007 - acc: 0.7518 - f1_m: 0.0116 - precision_m: 0.0250 - recall_m: 0.0076 - val_loss: 0.5568 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 71ms/step - loss: 0.5621 - acc: 0.7580 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5499 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.5675 - acc: 0.7580 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5405 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.5573 - acc: 0.7580 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5390 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.5562 - acc: 0.7580 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5405 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.523\n",
      "  Accuracy: 0.787\n",
      " fº 0.000\n",
      "Model: \"functional_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 218ms/step - loss: 0.6344 - acc: 0.7155 - f1_m: 0.0431 - precision_m: 0.0309 - recall_m: 0.0714 - val_loss: 0.5340 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 0.5211 - acc: 0.7580 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4710 - val_acc: 0.7705 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 0.4346 - acc: 0.8141 - f1_m: 0.3929 - precision_m: 0.7856 - recall_m: 0.3309 - val_loss: 0.3466 - val_acc: 0.8575 - val_f1_m: 0.6355 - val_precision_m: 0.8242 - val_recall_m: 0.5377\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 166ms/step - loss: 0.2829 - acc: 0.8879 - f1_m: 0.7344 - precision_m: 0.8593 - recall_m: 0.6611 - val_loss: 0.3120 - val_acc: 0.8816 - val_f1_m: 0.7689 - val_precision_m: 0.7089 - val_recall_m: 0.8470\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 0.2217 - acc: 0.9211 - f1_m: 0.8434 - precision_m: 0.8393 - recall_m: 0.8623 - val_loss: 0.2755 - val_acc: 0.8865 - val_f1_m: 0.7420 - val_precision_m: 0.7959 - val_recall_m: 0.7000\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 1s 169ms/step - loss: 0.1595 - acc: 0.9460 - f1_m: 0.8880 - precision_m: 0.8826 - recall_m: 0.8966 - val_loss: 0.2823 - val_acc: 0.8816 - val_f1_m: 0.7336 - val_precision_m: 0.7879 - val_recall_m: 0.6924\n",
      "Test set\n",
      "  Loss: 0.306\n",
      "  Accuracy: 0.865\n",
      " fº 0.631\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "\n",
    "#print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. RNN Applied to Category III - Informatives Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_5:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_5:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.7140 - acc: 0.6220 - f1_m: 0.0159 - precision_m: 0.0714 - recall_m: 0.0089WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_5:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "8/8 [==============================] - 1s 119ms/step - loss: 0.7140 - acc: 0.6220 - f1_m: 0.0159 - precision_m: 0.0714 - recall_m: 0.0089 - val_loss: 0.6640 - val_acc: 0.6208 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 0.6643 - acc: 0.6210 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6645 - val_acc: 0.6208 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.675\n",
      "  Accuracy: 0.602\n",
      " fº 0.000\n",
      "Model: \"functional_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 0.6623 - acc: 0.6033 - f1_m: 0.0725 - precision_m: 0.1739 - recall_m: 0.0877 - val_loss: 0.6052 - val_acc: 0.6280 - val_f1_m: 0.0655 - val_precision_m: 0.7500 - val_recall_m: 0.0352\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.5808 - acc: 0.7653 - f1_m: 0.5273 - precision_m: 0.8204 - recall_m: 0.4239 - val_loss: 0.5110 - val_acc: 0.7947 - val_f1_m: 0.6537 - val_precision_m: 0.8839 - val_recall_m: 0.5286\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.4028 - acc: 0.8629 - f1_m: 0.8063 - precision_m: 0.8913 - recall_m: 0.7472 - val_loss: 0.4450 - val_acc: 0.8213 - val_f1_m: 0.7145 - val_precision_m: 0.8992 - val_recall_m: 0.5998\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2866 - acc: 0.8879 - f1_m: 0.8438 - precision_m: 0.8949 - recall_m: 0.8074 - val_loss: 0.4212 - val_acc: 0.8430 - val_f1_m: 0.7564 - val_precision_m: 0.8724 - val_recall_m: 0.6863\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.2106 - acc: 0.9211 - f1_m: 0.8913 - precision_m: 0.9117 - recall_m: 0.8744 - val_loss: 0.4805 - val_acc: 0.8357 - val_f1_m: 0.7414 - val_precision_m: 0.8854 - val_recall_m: 0.6506\n",
      "Test set\n",
      "  Loss: 0.478\n",
      "  Accuracy: 0.829\n",
      " fº 0.753\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 3\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. RNN Applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "8/8 [==============================] - 2s 230ms/step - loss: 0.6284 - acc: 0.7072 - f1_m: 0.0471 - precision_m: 0.0316 - recall_m: 0.0929 - val_loss: 0.5318 - val_acc: 0.7609 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/8\n",
      "8/8 [==============================] - 1s 165ms/step - loss: 0.4951 - acc: 0.7601 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4463 - val_acc: 0.7609 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/8\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 0.3636 - acc: 0.8380 - f1_m: 0.4624 - precision_m: 0.8399 - recall_m: 0.3627 - val_loss: 0.3366 - val_acc: 0.8599 - val_f1_m: 0.6567 - val_precision_m: 0.9625 - val_recall_m: 0.5139\n",
      "Epoch 4/8\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 0.2506 - acc: 0.9107 - f1_m: 0.8004 - precision_m: 0.9039 - recall_m: 0.7471 - val_loss: 0.2811 - val_acc: 0.8889 - val_f1_m: 0.7823 - val_precision_m: 0.8494 - val_recall_m: 0.7276\n",
      "Epoch 5/8\n",
      "8/8 [==============================] - 1s 165ms/step - loss: 0.1466 - acc: 0.9481 - f1_m: 0.8868 - precision_m: 0.9146 - recall_m: 0.8639 - val_loss: 0.3109 - val_acc: 0.8841 - val_f1_m: 0.7438 - val_precision_m: 0.9143 - val_recall_m: 0.6323\n",
      "Test set\n",
      "  Loss: 0.331\n",
      "  Accuracy: 0.892\n",
      " fº 0.738\n",
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 200, 100)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 206,513\n",
      "Trainable params: 206,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_8:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_8:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.5983 - acc: 0.7210 - f1_m: 0.0704 - precision_m: 0.0481 - recall_m: 0.1310WARNING:tensorflow:Model was constructed with shape (None, 200) for input Tensor(\"inputs_8:0\", shape=(None, 200), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 0.5937 - acc: 0.7269 - f1_m: 0.0616 - precision_m: 0.0421 - recall_m: 0.1146 - val_loss: 0.5520 - val_acc: 0.7609 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.5548 - acc: 0.7601 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5736 - val_acc: 0.7609 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.588\n",
      "  Accuracy: 0.739\n",
      " fº 0.000\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=8,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 200\n",
    "#tok = Tokenizer(num_words=max_words)\n",
    "#tok.fit_on_texts(X1_train)\n",
    "#sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = X1_train\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc',f1_m,precision_m, recall_m])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.3,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "#test_sequences = X1_test\n",
    "test_sequences_matrix = X1_test\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(test_sequences_matrix, y1_test, verbose=0)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bi-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Bi-LSTM Applied to Category I - Tweets written by people suffering from eating disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 0.6927 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6937 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6923 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6935 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6924 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6922 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6939 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6925 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6937 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6922 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6938 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6921 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6941 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6921 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6940 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6922 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6937 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6923 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6936 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6920 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6943 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6921 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6942 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6921 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6946 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6920 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6950 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.6950 - acc: 0.4992 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.695\n",
      "  Accuracy: 0.499\n",
      " fº 0.000\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 2s 53ms/step - loss: 0.6881 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6816 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.6637 - acc: 0.5251 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6393 - val_acc: 0.4992 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.5816 - acc: 0.5425 - f1_m: 0.0796 - precision_m: 0.2443 - recall_m: 0.0571 - val_loss: 0.4970 - val_acc: 0.6717 - val_f1_m: 0.5095 - val_precision_m: 0.9178 - val_recall_m: 0.3760\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.4134 - acc: 0.8083 - f1_m: 0.7556 - precision_m: 0.9186 - recall_m: 0.6601 - val_loss: 0.4048 - val_acc: 0.8088 - val_f1_m: 0.7654 - val_precision_m: 0.9184 - val_recall_m: 0.6715\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.3269 - acc: 0.8656 - f1_m: 0.8449 - precision_m: 0.9129 - recall_m: 0.7966 - val_loss: 0.3811 - val_acc: 0.8257 - val_f1_m: 0.7980 - val_precision_m: 0.9067 - val_recall_m: 0.7283\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2766 - acc: 0.8882 - f1_m: 0.8760 - precision_m: 0.9271 - recall_m: 0.8436 - val_loss: 0.3809 - val_acc: 0.8494 - val_f1_m: 0.8300 - val_precision_m: 0.9081 - val_recall_m: 0.7781\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2289 - acc: 0.9020 - f1_m: 0.8926 - precision_m: 0.9459 - recall_m: 0.8519 - val_loss: 0.4272 - val_acc: 0.8223 - val_f1_m: 0.7900 - val_precision_m: 0.9103 - val_recall_m: 0.7181\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1992 - acc: 0.9216 - f1_m: 0.8913 - precision_m: 0.9257 - recall_m: 0.8653 - val_loss: 0.4269 - val_acc: 0.8342 - val_f1_m: 0.8124 - val_precision_m: 0.8860 - val_recall_m: 0.7667\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1676 - acc: 0.9375 - f1_m: 0.9059 - precision_m: 0.9372 - recall_m: 0.8828 - val_loss: 0.5107 - val_acc: 0.8223 - val_f1_m: 0.7877 - val_precision_m: 0.9042 - val_recall_m: 0.7171\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1414 - acc: 0.9448 - f1_m: 0.9402 - precision_m: 0.9675 - recall_m: 0.9183 - val_loss: 0.4637 - val_acc: 0.8494 - val_f1_m: 0.8392 - val_precision_m: 0.8700 - val_recall_m: 0.8258\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1280 - acc: 0.9506 - f1_m: 0.9451 - precision_m: 0.9699 - recall_m: 0.9264 - val_loss: 0.5700 - val_acc: 0.8308 - val_f1_m: 0.8066 - val_precision_m: 0.8844 - val_recall_m: 0.7588\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1118 - acc: 0.9535 - f1_m: 0.9507 - precision_m: 0.9659 - recall_m: 0.9392 - val_loss: 0.6177 - val_acc: 0.8409 - val_f1_m: 0.8199 - val_precision_m: 0.8946 - val_recall_m: 0.7711\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0922 - acc: 0.9637 - f1_m: 0.9345 - precision_m: 0.9582 - recall_m: 0.9171 - val_loss: 0.6182 - val_acc: 0.8376 - val_f1_m: 0.8236 - val_precision_m: 0.8847 - val_recall_m: 0.7830\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0755 - acc: 0.9724 - f1_m: 0.9684 - precision_m: 0.9837 - recall_m: 0.9551 - val_loss: 0.6408 - val_acc: 0.8511 - val_f1_m: 0.8424 - val_precision_m: 0.8623 - val_recall_m: 0.8358\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.6408 - acc: 0.8511 - f1_m: 0.8508 - precision_m: 0.8660 - recall_m: 0.8437\n",
      "Test set\n",
      "  Loss: 0.641\n",
      "  Accuracy: 0.851\n",
      " fº 0.851\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Bi-LSTM Applied to Category II - Tweets promotiong Eating Disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 3s 67ms/step - loss: 0.6581 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5754 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.5608 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5200 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5507 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5195 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5495 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5296 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5547 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5187 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.5517 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5258 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5530 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5191 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5493 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5203 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5498 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5182 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5523 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5188 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5537 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5201 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5501 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5214 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5496 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5202 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.5516 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5211 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.5211 - acc: 0.7868 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.521\n",
      "  Accuracy: 0.787\n",
      " fº 0.000\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 2s 51ms/step - loss: 0.6520 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5613 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.5034 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4479 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.4580 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4217 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.4098 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3873 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.3512 - acc: 0.7618 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.3402 - val_acc: 0.7868 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2837 - acc: 0.7691 - f1_m: 0.0480 - precision_m: 0.1591 - recall_m: 0.0292 - val_loss: 0.3030 - val_acc: 0.7970 - val_f1_m: 0.1096 - val_precision_m: 0.2667 - val_recall_m: 0.0728\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2269 - acc: 0.8548 - f1_m: 0.5227 - precision_m: 0.8395 - recall_m: 0.4170 - val_loss: 0.3151 - val_acc: 0.8274 - val_f1_m: 0.3916 - val_precision_m: 0.6028 - val_recall_m: 0.3231\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1830 - acc: 0.9143 - f1_m: 0.7794 - precision_m: 0.9080 - recall_m: 0.7140 - val_loss: 0.3246 - val_acc: 0.8680 - val_f1_m: 0.6364 - val_precision_m: 0.7286 - val_recall_m: 0.5971\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1545 - acc: 0.9339 - f1_m: 0.8164 - precision_m: 0.8959 - recall_m: 0.7668 - val_loss: 0.3502 - val_acc: 0.8748 - val_f1_m: 0.6683 - val_precision_m: 0.7365 - val_recall_m: 0.6526\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1282 - acc: 0.9492 - f1_m: 0.8570 - precision_m: 0.9081 - recall_m: 0.8328 - val_loss: 0.4070 - val_acc: 0.8579 - val_f1_m: 0.5639 - val_precision_m: 0.6939 - val_recall_m: 0.5029\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1090 - acc: 0.9579 - f1_m: 0.8870 - precision_m: 0.9290 - recall_m: 0.8607 - val_loss: 0.4113 - val_acc: 0.8748 - val_f1_m: 0.6458 - val_precision_m: 0.7128 - val_recall_m: 0.6143\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0907 - acc: 0.9702 - f1_m: 0.9167 - precision_m: 0.9360 - recall_m: 0.9066 - val_loss: 0.4724 - val_acc: 0.8714 - val_f1_m: 0.6287 - val_precision_m: 0.7244 - val_recall_m: 0.5887\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0822 - acc: 0.9746 - f1_m: 0.9379 - precision_m: 0.9527 - recall_m: 0.9383 - val_loss: 0.5050 - val_acc: 0.8680 - val_f1_m: 0.6370 - val_precision_m: 0.6908 - val_recall_m: 0.6362\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0713 - acc: 0.9753 - f1_m: 0.9208 - precision_m: 0.9353 - recall_m: 0.9181 - val_loss: 0.5368 - val_acc: 0.8646 - val_f1_m: 0.6036 - val_precision_m: 0.7013 - val_recall_m: 0.5710\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5368 - acc: 0.8646 - f1_m: 0.6563 - precision_m: 0.7735 - recall_m: 0.6071\n",
      "Test set\n",
      "  Loss: 0.537\n",
      "  Accuracy: 0.865\n",
      " fº 0.656\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Bi-LSTM Applied to Category III - Informatives Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 3s 69ms/step - loss: 0.6793 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6725 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6648 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6727 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6644 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6740 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6645 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6724 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6640 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6752 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6642 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6729 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6645 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6723 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6637 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6734 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 27ms/step - loss: 0.6651 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6726 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.6642 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6734 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.6640 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6726 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 29ms/step - loss: 0.6646 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6723 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6645 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6721 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 28ms/step - loss: 0.6646 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6724 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6724 - acc: 0.6024 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.672\n",
      "  Accuracy: 0.602\n",
      " fº 0.000\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 2s 54ms/step - loss: 0.6834 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6763 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.6553 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6588 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.6005 - acc: 0.6209 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5648 - val_acc: 0.6024 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.4605 - acc: 0.7197 - f1_m: 0.3394 - precision_m: 0.5925 - recall_m: 0.2620 - val_loss: 0.4649 - val_acc: 0.8342 - val_f1_m: 0.7555 - val_precision_m: 0.8751 - val_recall_m: 0.6849\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.3469 - acc: 0.8722 - f1_m: 0.7941 - precision_m: 0.8579 - recall_m: 0.7559 - val_loss: 0.4453 - val_acc: 0.8426 - val_f1_m: 0.7886 - val_precision_m: 0.8303 - val_recall_m: 0.7743\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.2737 - acc: 0.8983 - f1_m: 0.8310 - precision_m: 0.9028 - recall_m: 0.7837 - val_loss: 0.4820 - val_acc: 0.8376 - val_f1_m: 0.7760 - val_precision_m: 0.8423 - val_recall_m: 0.7420\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.2233 - acc: 0.9150 - f1_m: 0.8500 - precision_m: 0.9091 - recall_m: 0.8081 - val_loss: 0.5227 - val_acc: 0.8257 - val_f1_m: 0.7636 - val_precision_m: 0.8307 - val_recall_m: 0.7341\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.1825 - acc: 0.9383 - f1_m: 0.8892 - precision_m: 0.9316 - recall_m: 0.8561 - val_loss: 0.6165 - val_acc: 0.8223 - val_f1_m: 0.7541 - val_precision_m: 0.8254 - val_recall_m: 0.7202\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.1444 - acc: 0.9499 - f1_m: 0.9294 - precision_m: 0.9631 - recall_m: 0.9014 - val_loss: 0.6758 - val_acc: 0.8173 - val_f1_m: 0.7605 - val_precision_m: 0.7912 - val_recall_m: 0.7645\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.1326 - acc: 0.9513 - f1_m: 0.9103 - precision_m: 0.9459 - recall_m: 0.8820 - val_loss: 0.7483 - val_acc: 0.8257 - val_f1_m: 0.7764 - val_precision_m: 0.7837 - val_recall_m: 0.7995\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.1232 - acc: 0.9542 - f1_m: 0.9130 - precision_m: 0.9432 - recall_m: 0.8902 - val_loss: 0.7449 - val_acc: 0.8173 - val_f1_m: 0.7469 - val_precision_m: 0.8064 - val_recall_m: 0.7236\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.0901 - acc: 0.9673 - f1_m: 0.9338 - precision_m: 0.9561 - recall_m: 0.9160 - val_loss: 0.8673 - val_acc: 0.8240 - val_f1_m: 0.7611 - val_precision_m: 0.8096 - val_recall_m: 0.7452\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.0725 - acc: 0.9760 - f1_m: 0.9685 - precision_m: 0.9913 - recall_m: 0.9484 - val_loss: 1.0060 - val_acc: 0.8240 - val_f1_m: 0.7657 - val_precision_m: 0.8024 - val_recall_m: 0.7631\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 0s 11ms/step - loss: 0.0645 - acc: 0.9797 - f1_m: 0.9490 - precision_m: 0.9676 - recall_m: 0.9336 - val_loss: 1.0905 - val_acc: 0.8190 - val_f1_m: 0.7592 - val_precision_m: 0.7876 - val_recall_m: 0.7640\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 1.0905 - acc: 0.8190 - f1_m: 0.7616 - precision_m: 0.7714 - recall_m: 0.7599\n",
      "Test set\n",
      "  Loss: 1.090\n",
      "  Accuracy: 0.819\n",
      " fº 0.762\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 3\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Bi-LSTM Applied to Category IV - Scientific Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "44/44 [==============================] - 3s 71ms/step - loss: 0.6225 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5743 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 1s 30ms/step - loss: 0.5527 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5753 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5523 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5744 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5512 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5780 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5526 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5744 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5516 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5746 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 1s 32ms/step - loss: 0.5521 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5746 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5546 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5739 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5528 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5800 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5533 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5754 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5511 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5745 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 1s 32ms/step - loss: 0.5519 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5737 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 1s 33ms/step - loss: 0.5545 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5771 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 1s 31ms/step - loss: 0.5516 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5750 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.5750 - acc: 0.7394 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Test set\n",
      "  Loss: 0.575\n",
      "  Accuracy: 0.739\n",
      " fº 0.000\n",
      "Epoch 1/14\n",
      "44/44 [==============================] - 2s 51ms/step - loss: 0.6656 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.6291 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.5713 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5756 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 3/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.5155 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.5520 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 4/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.4345 - acc: 0.7603 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4279 - val_acc: 0.7394 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.2887 - acc: 0.8453 - f1_m: 0.4076 - precision_m: 0.5606 - recall_m: 0.3463 - val_loss: 0.3165 - val_acc: 0.8951 - val_f1_m: 0.7078 - val_precision_m: 0.8322 - val_recall_m: 0.6480\n",
      "Epoch 6/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1964 - acc: 0.9354 - f1_m: 0.8141 - precision_m: 0.8812 - recall_m: 0.7701 - val_loss: 0.2814 - val_acc: 0.9103 - val_f1_m: 0.7929 - val_precision_m: 0.8106 - val_recall_m: 0.8278\n",
      "Epoch 7/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1389 - acc: 0.9601 - f1_m: 0.8866 - precision_m: 0.9320 - recall_m: 0.8560 - val_loss: 0.3204 - val_acc: 0.9002 - val_f1_m: 0.7351 - val_precision_m: 0.8204 - val_recall_m: 0.6911\n",
      "Epoch 8/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.1072 - acc: 0.9666 - f1_m: 0.9044 - precision_m: 0.9424 - recall_m: 0.8796 - val_loss: 0.3527 - val_acc: 0.9036 - val_f1_m: 0.7577 - val_precision_m: 0.8091 - val_recall_m: 0.7592\n",
      "Epoch 9/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0826 - acc: 0.9724 - f1_m: 0.9402 - precision_m: 0.9695 - recall_m: 0.9186 - val_loss: 0.4086 - val_acc: 0.9019 - val_f1_m: 0.7700 - val_precision_m: 0.8440 - val_recall_m: 0.7363\n",
      "Epoch 10/14\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.0649 - acc: 0.9804 - f1_m: 0.9592 - precision_m: 0.9767 - recall_m: 0.9467 - val_loss: 0.4349 - val_acc: 0.9019 - val_f1_m: 0.7659 - val_precision_m: 0.8151 - val_recall_m: 0.7619\n",
      "Epoch 11/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0543 - acc: 0.9826 - f1_m: 0.9417 - precision_m: 0.9539 - recall_m: 0.9355 - val_loss: 0.5186 - val_acc: 0.8985 - val_f1_m: 0.7641 - val_precision_m: 0.8440 - val_recall_m: 0.7266\n",
      "Epoch 12/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0481 - acc: 0.9862 - f1_m: 0.9675 - precision_m: 0.9914 - recall_m: 0.9491 - val_loss: 0.4989 - val_acc: 0.9036 - val_f1_m: 0.7600 - val_precision_m: 0.8091 - val_recall_m: 0.7619\n",
      "Epoch 13/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0453 - acc: 0.9862 - f1_m: 0.9432 - precision_m: 0.9515 - recall_m: 0.9408 - val_loss: 0.6253 - val_acc: 0.8883 - val_f1_m: 0.7061 - val_precision_m: 0.8105 - val_recall_m: 0.6510\n",
      "Epoch 14/14\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 0.0365 - acc: 0.9906 - f1_m: 0.9546 - precision_m: 0.9646 - recall_m: 0.9483 - val_loss: 0.5635 - val_acc: 0.9052 - val_f1_m: 0.7685 - val_precision_m: 0.8252 - val_recall_m: 0.7572\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 0.5635 - acc: 0.9052 - f1_m: 0.7774 - precision_m: 0.8314 - recall_m: 0.7425\n",
      "Test set\n",
      "  Loss: 0.563\n",
      "  Accuracy: 0.905\n",
      " fº 0.777\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=1968,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))\n",
    "\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_orig, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(X1_test,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n fº {:0.3f}'.format(loss,accuracy,f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
